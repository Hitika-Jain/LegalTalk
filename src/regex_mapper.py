# -*- coding: utf-8 -*-
"""regex_mapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16p1-RLac6aOYI1wLGkeN3IvcTi_QoMnW
"""

import re
import json
import csv
from pathlib import Path
from src.utils import normalize_to_ipc

# 1. Build improved regex patterns
# =============================

def build_statute_patterns():
    """
    Build regex patterns to extract statute references like:
      302
      376(2)(g)
      120-B
      120B
      234(b), 234-b, 234b
    without truncation.
    """

    flags = re.IGNORECASE | re.UNICODE

    # Full statute-id pattern
    STATUTE_ID = r'[0-9]{1,4}(?:\([0-9A-Za-z]+\)|-[A-Za-z]|[A-Za-z]){0,3}'

    patterns = []

    # Section ... pattern
    patterns.append((
        "section_keyword",
        re.compile(
            rf'\b(?:section|sections|sec|secs|s|§)\.?\s*[:\s]*({STATUTE_ID})',
            flags
        )
    ))

    # IPC prefix
    patterns.append((
        "ipc_prefix",
        re.compile(
            rf'\b(?:ipc|indian\s+penal\s+code)[\s_:,\-]*({STATUTE_ID})',
            flags
        )
    ))

    # Section X of/under IPC
    patterns.append((
        "section_of_ipc",
        re.compile(
            rf'\bsections?\s*({STATUTE_ID})\s*(?:of|under)\s*(?:the\s*)?(?:ipc|indian\s+penal\s+code)\b',
            flags
        )
    ))

    # X IPC
    patterns.append((
        "num_then_ipc",
        re.compile(
            rf'({STATUTE_ID})\s*(?:,?\s*ipc|,?\s*indian\s+penal\s+code)\b',
            flags
        )
    ))

    # Articles
    patterns.append((
        "article",
        re.compile(
            r'\b(?:article|art)\.?\s*[:\s]*([0-9]{1,3}(?:[A-Za-z]|\([0-9A-Za-z]+\))?)',
            flags
        )
    ))

    # Bare fallback
    patterns.append((
        "bare_number_with_letter",
        re.compile(r'\b([0-9]{1,4}[A-Za-z])\b', flags)
    ))

    return patterns

# 2. Text normalization
# =============================

ZERO_WIDTH = ['\u200b', '\u200c', '\u200d', '\ufeff']

def normalize_text_for_matching(s: str):
    if s is None:
        return ""
    s2 = s.replace('\u00A0', ' ')  # NBSP to space
    for z in ZERO_WIDTH:
        s2 = s2.replace(z, '')
    s2 = re.sub(r'\s+', ' ', s2)
    return s2

# 3. Find statutes in a given sentence
# =============================

def find_statutes_in_text(text, patterns=None):
    """
    Return list of matches:
        {
            pattern_name,
            match_text,
            statute_raw,
            span_start,
            span_end
        }
    """

    if text is None:
        return []

    patterns = patterns or build_statute_patterns()

    s = text
    norm = normalize_text_for_matching(s)

    results = []

    # We search on normalized, but span mapping happens on raw
    for pname, pat in patterns:
        for m in pat.finditer(norm):
            raw_group = m.group(1)
            matched_norm = m.group(0)

            # Find same substring in original text
            idx = s.find(raw_group)
            if idx == -1:
                # fallback: try case-insensitive search
                lower_raw = raw_group.lower()
                idx = s.lower().find(lower_raw)

            if idx != -1:
                results.append({
                    "pattern_name": pname,
                    "match_text": matched_norm,
                    "statute_raw": raw_group,
                    "span_start": idx,
                    "span_end": idx + len(raw_group)
                })

    return results

# 4. Apply to JSONL file
# =============================
import re, json, csv
from pathlib import Path
from src.utils import normalize_to_ipc

# (assume build_statute_patterns and find_statutes_in_text are defined here above in your notebook)
# If they are in other cells, keep those definitions; this file focuses on map_jsonl_file_to_hits.

def map_jsonl_file_to_hits(jsonl_path, out_csv, include_legal_terms=True, filter_none_canonical=True):
    """
    Reads parsed JSONL and writes CSV including canonical_id (IPC_...).
    """
    jsonl_path = str(jsonl_path)
    out_csv = str(out_csv)

    # import local build/find if they exist; otherwise the notebook-level functions will be used during runtime.
    try:
        from src.regex_mapper import build_statute_patterns as _bp  # guard to avoid recursion when reloading
        patterns = _bp()
    except Exception:
        # Patterns will be taken from notebook definitions if present
        patterns = None

    rows = []
    with open(jsonl_path, "r", encoding="utf-8") as fin:
        for line_no, line in enumerate(fin):
            rec = json.loads(line)
            txt = rec.get("text", "") or ""
            page = rec.get("page")
            line_id = rec.get("line_id")
            start_char = rec.get("start_char")

            # call notebook-level find_statutes_in_text if module-level isn't available
            try:
                from src.regex_mapper import find_statutes_in_text as finder
                hits = finder(txt, patterns=patterns)
            except Exception:
                # fallback to expecting find_statutes_in_text defined in the notebook's global scope
                hits = find_statutes_in_text(txt, patterns=patterns)

            for h in hits:
                statute_raw = h.get("statute_raw") or h.get("match_text")
                canonical_id = normalize_to_ipc(statute_raw)

                if filter_none_canonical and canonical_id is None:
                    continue

                row = {
                    "line_no": line_no,
                    "page": page,
                    "line_id": line_id,
                    "sentence_text": txt,
                    "statute_raw": statute_raw,
                    "canonical_id": canonical_id,
                    "pattern_name": h.get("pattern_name"),
                    "match_text": h.get("match_text"),
                    "local_start": h.get("span_start"),
                    "local_end": h.get("span_end"),
                    "global_start": (start_char + h.get("span_start")) if isinstance(start_char, int) and h.get("span_start") is not None else None,
                    "global_end": (start_char + h.get("span_end")) if isinstance(start_char, int) and h.get("span_end") is not None else None
                }
                rows.append(row)

    # write CSV
    fieldnames = ["line_no","page","line_id","sentence_text",
                  "statute_raw","canonical_id","pattern_name","match_text",
                  "local_start","local_end","global_start","global_end"]
    with open(out_csv, "w", encoding="utf-8", newline="") as fout:
        writer = csv.DictWriter(fout, fieldnames=fieldnames)
        writer.writeheader()
        for r in rows:
            writer.writerow({k: r.get(k, "") for k in fieldnames})
    print(f"Wrote {len(rows)} hits → {out_csv}")
    return rows

#from google.colab import drive
#drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#!git clone https://github.com/Hitika-Jain/LegalTalk.git

# %cd LegalTalk

