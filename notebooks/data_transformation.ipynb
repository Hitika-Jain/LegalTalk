{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13ke6PtjFYkhna7komHquTyj-__x6hcs9",
      "authorship_tag": "ABX9TyNC4Eb1NDOg8IdtRB1kiuSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hitika-Jain/LegalTalk/blob/main/notebooks/data_transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pt-6-nHiYhU",
        "outputId": "52510c29-250e-4c88-b729-1b881f8e47a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LegalTalk'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 77 (delta 30), reused 24 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (77/77), 473.01 KiB | 4.93 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "/content/LegalTalk\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Hitika-Jain/LegalTalk.git\n",
        "\n",
        "%cd LegalTalk\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scripts/convert_labels_to_canonical.py\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "STATUTES_CSV = \"/content/drive/MyDrive/legal_dataset/statutes-00000-of-00001.csv\"   # contains canonical IDs like IPC_302 in a column e.g. 'id' or 'statute_id'\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/legal_dataset/train-00000-of-00001.csv\"\n",
        "DEV_CSV = \"/content/drive/MyDrive/legal_dataset/dev-00000-of-00001 (1).csv\"\n",
        "TEST_CSV = \"/content/drive/MyDrive/legal_dataset/test-00000-of-00001.csv\"  # set path if you have a test file\n",
        "OUT_DIR = Path(\"/content/converted_labels\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# If your filesystem/statutes index is 1-based (labels start at 1 -> first row), set to 1\n",
        "# If unsure, leave as None to auto-detect\n",
        "ASSUME_INDEX_BASE = None  # set to 0 or 1 to force\n",
        "# ============================\n",
        "\n",
        "def read_statutes(path):\n",
        "    df = pd.read_csv(path, dtype=str).fillna('')\n",
        "    # pick canonical id column heuristically\n",
        "    for cand in ('id','statute_id','canonical_id','code','label'):\n",
        "        if cand in df.columns:\n",
        "            return df, cand\n",
        "    # otherwise return first column as canonical id column\n",
        "    return df, df.columns[0]\n",
        "\n",
        "def parse_label_cell(cell):\n",
        "    \"\"\"Return list of tokens (strings) from many label formats.\"\"\"\n",
        "    if pd.isna(cell) or str(cell).strip() == \"\":\n",
        "        return []\n",
        "    s = str(cell).strip()\n",
        "    # try python literal\n",
        "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
        "        try:\n",
        "            parsed = ast.literal_eval(s)\n",
        "            if isinstance(parsed, (list, tuple)):\n",
        "                return [str(x).strip() for x in parsed if str(x).strip()!='']\n",
        "        except Exception:\n",
        "            pass\n",
        "    # if contains semicolon or comma separate\n",
        "    if \";\" in s:\n",
        "        toks = [t.strip().strip(\"'\\\"\") for t in s.split(\";\") if t.strip()]\n",
        "        return toks\n",
        "    if \",\" in s and not re.fullmatch(r'^\\[\\s*\\d+(?:\\s+\\d+)*\\s*\\]$', s):  # avoid \"[69  9  3]\" false comma\n",
        "        toks = [t.strip().strip(\"'\\\"\") for t in s.split(\",\") if t.strip()]\n",
        "        return toks\n",
        "    # fallback: extract tokens (either words like IPC_302 or numbers)\n",
        "    # treat sequences like \"69  9  3\" -> ['69','9','3']\n",
        "    splitted = re.split(r'\\s+', re.sub(r'[\\[\\]\\(\\)\\,;]+',' ', s)).strip() if False else re.split(r'\\s+', re.sub(r'[\\[\\]\\(\\)\\,;]+',' ', s))\n",
        "    toks = [t.strip().strip(\"'\\\"\") for t in splitted if t.strip()]\n",
        "    return toks\n",
        "\n",
        "def build_mappings(stat_df, id_col, index_base_guess=0):\n",
        "    \"\"\"Return index->ipc mapping and digit->ipc mapping (from numeric suffix).\"\"\"\n",
        "    index_to_ipc = {}\n",
        "    digit_to_ipc = {}\n",
        "    for idx, row in stat_df.reset_index().iterrows():\n",
        "        ipc = str(row[id_col]).strip()\n",
        "        # Map index token according to index_base_guess (0 or 1)\n",
        "        index_to_ipc[idx + index_base_guess] = ipc\n",
        "        # map numeric suffix, e.g., IPC_302 -> '302'\n",
        "        m = re.search(r'(\\d+)', ipc)\n",
        "        if m:\n",
        "            digit_to_ipc[m.group(1)] = ipc\n",
        "        digit_to_ipc[ipc] = ipc\n",
        "    return index_to_ipc, digit_to_ipc\n",
        "\n",
        "def try_detect_index_base(sample_labels, index_to_ipc_len):\n",
        "    \"\"\"\n",
        "    Heuristics:\n",
        "     - If any numeric token > len(index_to_ipc) and <= len+1 then probably 1-based.\n",
        "     - If all numeric tokens <= len(index_to_ipc) maybe 0-based.\n",
        "    Returns 0 or 1 (best guess).\n",
        "    \"\"\"\n",
        "    nums = []\n",
        "    for lab in sample_labels:\n",
        "        for tok in lab:\n",
        "            if tok.isdigit():\n",
        "                nums.append(int(tok))\n",
        "    if not nums:\n",
        "        return 0  # default 0 if no numeric tokens found\n",
        "\n",
        "    max_num = max(nums)\n",
        "    # if max exceeds length and equals length -> likely 1-based\n",
        "    if max_num > index_to_ipc_len and max_num <= index_to_ipc_len + 1:\n",
        "        return 1\n",
        "    # if smallest numeric token is 1 and many are >=1, could be 1-based\n",
        "    if min(nums) >= 1 and max_num <= index_to_ipc_len:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def convert_df_labels(df, index_to_ipc, digit_to_ipc, index_base):\n",
        "    out_rows = []\n",
        "    unmapped = Counter()\n",
        "    for _, r in df.iterrows():\n",
        "        text_col = None\n",
        "        for c in ['text','sentence_text','document_text','content']:\n",
        "            if c in df.columns:\n",
        "                text_col = c\n",
        "                break\n",
        "        if text_col is None:\n",
        "            # fallback: pick first non-id column\n",
        "            cols = list(df.columns)\n",
        "            if len(cols) >= 2:\n",
        "                text_col = cols[1]\n",
        "            else:\n",
        "                raise ValueError(\"No text column found.\")\n",
        "        text = r[text_col]\n",
        "        raw_labels = r.get('labels') or r.get('label') or r.get('labels_raw') or ''\n",
        "        toks = parse_label_cell(raw_labels)\n",
        "\n",
        "        mapped = []\n",
        "        for t in toks:\n",
        "            t_str = str(t).strip()\n",
        "            if t_str == '':\n",
        "                continue\n",
        "            # numeric?\n",
        "            if re.fullmatch(r'\\d+', t_str):\n",
        "                n = int(t_str)\n",
        "                # try digit->ipc mapping first (if legends like '302' -> IPC_302)\n",
        "                ipc_from_digit = digit_to_ipc.get(str(n))\n",
        "                if ipc_from_digit:\n",
        "                    mapped.append(ipc_from_digit)\n",
        "                    continue\n",
        "                # else use index mapping (apply index_base)\n",
        "                key = n - index_base\n",
        "                ipc_from_index = index_to_ipc.get(key)\n",
        "                if ipc_from_index:\n",
        "                    mapped.append(ipc_from_index)\n",
        "                    continue\n",
        "                # fallback: try direct index if present\n",
        "                ipc_from_index2 = index_to_ipc.get(n)\n",
        "                if ipc_from_index2:\n",
        "                    mapped.append(ipc_from_index2)\n",
        "                    continue\n",
        "                unmapped[t_str] += 1\n",
        "                mapped.append(t_str)  # keep raw for inspection\n",
        "            else:\n",
        "                # maybe already canonical like IPC_302\n",
        "                # try digit extraction\n",
        "                m = re.search(r'(\\d+)', t_str)\n",
        "                if t_str in digit_to_ipc:\n",
        "                    mapped.append(digit_to_ipc[t_str])\n",
        "                elif m and m.group(1) in digit_to_ipc:\n",
        "                    mapped.append(digit_to_ipc[m.group(1)])\n",
        "                else:\n",
        "                    # unknown token - keep as-is but count\n",
        "                    unmapped[t_str] += 1\n",
        "                    mapped.append(t_str)\n",
        "        # unique preserve order\n",
        "        seen = set()\n",
        "        final = []\n",
        "        for x in mapped:\n",
        "            if x not in seen:\n",
        "                final.append(x); seen.add(x)\n",
        "        out_rows.append({'text': text, 'labels': ';'.join(final)})\n",
        "    return pd.DataFrame(out_rows), unmapped\n",
        "\n",
        "def process_file(in_path, out_path, index_to_ipc, digit_to_ipc, index_base):\n",
        "    df = pd.read_csv(in_path, dtype=str).fillna('')\n",
        "    conv_df, unmapped = convert_df_labels(df, index_to_ipc, digit_to_ipc, index_base)\n",
        "    conv_df.to_csv(out_path, index=False)\n",
        "    return conv_df, unmapped\n",
        "\n",
        "# ========== run ==========\n",
        "stat_df, id_col = read_statutes(STATUTES_CSV)\n",
        "# Build index mapping using both possible index bases to inspect\n",
        "index_to_ipc_0, digit_to_ipc_0 = build_mappings(stat_df, id_col, index_base_guess=0)\n",
        "index_to_ipc_1, digit_to_ipc_1 = build_mappings(stat_df, id_col, index_base_guess=1)\n",
        "\n",
        "# Take a small sample from train to detect base, else use ASSUME_INDEX_BASE\n",
        "sample_df = pd.read_csv(TRAIN_CSV, dtype=str, nrows=200).fillna('')\n",
        "sample_labels = [parse_label_cell(x) for x in sample_df.get('labels', sample_df.columns[-1])]\n",
        "if ASSUME_INDEX_BASE is None:\n",
        "    guess = try_detect_index_base(sample_labels, len(index_to_ipc_0))\n",
        "    print(\"Auto-detected index_base:\", guess)\n",
        "    index_base_used = guess\n",
        "else:\n",
        "    index_base_used = ASSUME_INDEX_BASE\n",
        "    print(\"Forced index_base:\", index_base_used)\n",
        "\n",
        "index_to_ipc = index_to_ipc_0 if index_base_used == 0 else index_to_ipc_1\n",
        "digit_to_ipc = digit_to_ipc_0 if index_base_used == 0 else digit_to_ipc_1\n",
        "\n",
        "# Process train/dev/test\n",
        "print(\"Processing train ->\", OUT_DIR / 'train_converted.csv')\n",
        "train_conv, train_unmapped = process_file(TRAIN_CSV, OUT_DIR / 'train_converted.csv', index_to_ipc, digit_to_ipc, index_base_used)\n",
        "print(\"Unmapped tokens in train (sample):\", dict(list(train_unmapped.items())[:20]))\n",
        "\n",
        "if DEV_CSV:\n",
        "    print(\"Processing dev ->\", OUT_DIR / 'dev_converted.csv')\n",
        "    dev_conv, dev_unmapped = process_file(DEV_CSV, OUT_DIR / 'dev_converted.csv', index_to_ipc, digit_to_ipc, index_base_used)\n",
        "    print(\"Unmapped tokens in dev (sample):\", dict(list(dev_unmapped.items())[:20]))\n",
        "\n",
        "if TEST_CSV:\n",
        "    print(\"Processing test ->\", OUT_DIR / 'test_converted.csv')\n",
        "    test_conv, test_unmapped = process_file(TEST_CSV, OUT_DIR / 'test_converted.csv', index_to_ipc, digit_to_ipc, index_base_used)\n",
        "    print(\"Unmapped tokens in test (sample):\", dict(list(test_unmapped.items())[:20]))\n",
        "\n",
        "# Quick label distribution check\n",
        "all_labels = ';'.join(train_conv['labels'].astype(str)).split(';')\n",
        "print(\"Top labels in train:\")\n",
        "from collections import Counter\n",
        "print(Counter([l for l in all_labels if l]).most_common(30))\n",
        "print(\"Done. Converted CSVs are in:\", OUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aASUXu0g-UjK",
        "outputId": "adbb5903-7555-4be4-809d-9823a7efc41c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-detected index_base: 1\n",
            "Processing train -> /content/converted_labels/train_converted.csv\n",
            "Unmapped tokens in train (sample): {'0': 343}\n",
            "Processing dev -> /content/converted_labels/dev_converted.csv\n",
            "Unmapped tokens in dev (sample): {'0': 86}\n",
            "Processing test -> /content/converted_labels/test_converted.csv\n",
            "Unmapped tokens in test (sample): {'0': 117}\n",
            "Top labels in train:\n",
            "[('Section 5', 11730), ('Section 34', 11151), ('Section 500', 7880), ('Section 313', 7728), ('Section 304B', 7120), ('Section 13', 4805), ('Section 120B', 4324), ('Section 114', 4134), ('Section 417', 4046), ('Section 494', 3835), ('Section 147', 3818), ('Section 366A', 3723), ('Section 320', 3660), ('Section 337', 3184), ('Section 300', 3157), ('Section 229A', 3009), ('Section 323', 2618), ('Section 324', 2509), ('Section 395', 2303), ('Section 342', 2204), ('Section 465', 2113), ('Section 467', 2066), ('Section 498A', 1973), ('Section 193', 1909), ('Section 353', 1787), ('Section 419', 1739), ('Section 304A', 1713), ('Section 457', 1661), ('Section 376', 1658), ('Section 448', 1549)]\n",
            "Done. Converted CSVs are in: /content/converted_labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_labels_for_training.py\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from src.utils import normalize_to_ipc   # use your utils if present\n",
        "import re\n",
        "\n",
        "IN_CSV = \"/content/converted_labels/train_converted.csv\"   # your converted file (text, labels)\n",
        "OUT_CSV = \"/content/converted_labels/train_canonical.csv\"\n",
        "\n",
        "df = pd.read_csv(IN_CSV, dtype=str).fillna('')\n",
        "# preserve raw\n",
        "df['labels_raw'] = df['labels'].astype(str)\n",
        "\n",
        "# helper to split semicolon separated raw labels into tokens\n",
        "def split_raw_labels(s):\n",
        "    if pd.isna(s) or str(s).strip()=='':\n",
        "        return []\n",
        "    toks = [t.strip() for t in re.split(r'[;,\\|]', str(s)) if t.strip()]\n",
        "    return toks\n",
        "\n",
        "# map tokens -> canonical using normalize_to_ipc\n",
        "def map_tokens_to_canonical(tokens):\n",
        "    mapped = []\n",
        "    unmapped = []\n",
        "    for t in tokens:\n",
        "        # first try if token already looks canonical (IPC_)\n",
        "        if isinstance(t, str) and t.upper().startswith('IPC_'):\n",
        "            mapped.append(t.upper())\n",
        "            continue\n",
        "        # use normalize_to_ipc which converts things like 'Section 120-B' or '120' -> 'IPC_120B' etc\n",
        "        cand = normalize_to_ipc(t)\n",
        "        if cand:\n",
        "            mapped.append(cand)\n",
        "        else:\n",
        "            unmapped.append(t)\n",
        "    # unique preserve order\n",
        "    seen = set(); out=[]\n",
        "    for m in mapped:\n",
        "        if m not in seen: out.append(m); seen.add(m)\n",
        "    return out, unmapped\n",
        "\n",
        "all_unmapped = []\n",
        "labels_canonical = []\n",
        "for _, row in df.iterrows():\n",
        "    toks = split_raw_labels(row['labels_raw'])\n",
        "    mapped, unmapped = map_tokens_to_canonical(toks)\n",
        "    labels_canonical.append(';'.join(mapped))\n",
        "    if unmapped:\n",
        "        all_unmapped.extend(unmapped)\n",
        "\n",
        "df['labels_canonical'] = labels_canonical\n",
        "\n",
        "# save\n",
        "Path(OUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(\"Wrote:\", OUT_CSV)\n",
        "print(\"Total rows:\", len(df))\n",
        "print(\"Unique unmapped tokens (sample up to 50):\", list(dict.fromkeys(all_unmapped))[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9iOMszNBnd_",
        "outputId": "6f84e8b0-905e-479e-b991-9eeea7e1ffa0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: /content/converted_labels/train_canonical.csv\n",
            "Total rows: 42750\n",
            "Unique unmapped tokens (sample up to 50): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_labels_for_training.py\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from src.utils import normalize_to_ipc   # use your utils if present\n",
        "import re\n",
        "\n",
        "IN_CSV = \"/content/converted_labels/test_converted.csv\"   # your converted file (text, labels)\n",
        "OUT_CSV = \"/content/converted_labels/test_canonical.csv\"\n",
        "\n",
        "df = pd.read_csv(IN_CSV, dtype=str).fillna('')\n",
        "# preserve raw\n",
        "df['labels_raw'] = df['labels'].astype(str)\n",
        "\n",
        "# helper to split semicolon separated raw labels into tokens\n",
        "def split_raw_labels(s):\n",
        "    if pd.isna(s) or str(s).strip()=='':\n",
        "        return []\n",
        "    toks = [t.strip() for t in re.split(r'[;,\\|]', str(s)) if t.strip()]\n",
        "    return toks\n",
        "\n",
        "# map tokens -> canonical using normalize_to_ipc\n",
        "def map_tokens_to_canonical(tokens):\n",
        "    mapped = []\n",
        "    unmapped = []\n",
        "    for t in tokens:\n",
        "        # first try if token already looks canonical (IPC_)\n",
        "        if isinstance(t, str) and t.upper().startswith('IPC_'):\n",
        "            mapped.append(t.upper())\n",
        "            continue\n",
        "        # use normalize_to_ipc which converts things like 'Section 120-B' or '120' -> 'IPC_120B' etc\n",
        "        cand = normalize_to_ipc(t)\n",
        "        if cand:\n",
        "            mapped.append(cand)\n",
        "        else:\n",
        "            unmapped.append(t)\n",
        "    # unique preserve order\n",
        "    seen = set(); out=[]\n",
        "    for m in mapped:\n",
        "        if m not in seen: out.append(m); seen.add(m)\n",
        "    return out, unmapped\n",
        "\n",
        "all_unmapped = []\n",
        "labels_canonical = []\n",
        "for _, row in df.iterrows():\n",
        "    toks = split_raw_labels(row['labels_raw'])\n",
        "    mapped, unmapped = map_tokens_to_canonical(toks)\n",
        "    labels_canonical.append(';'.join(mapped))\n",
        "    if unmapped:\n",
        "        all_unmapped.extend(unmapped)\n",
        "\n",
        "df['labels_canonical'] = labels_canonical\n",
        "\n",
        "# save\n",
        "Path(OUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(\"Wrote:\", OUT_CSV)\n",
        "print(\"Total rows:\", len(df))\n",
        "print(\"Unique unmapped tokens (sample up to 50):\", list(dict.fromkeys(all_unmapped))[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeISmA5mB0FM",
        "outputId": "44baff39-c886-4dbd-abe4-4affa6ac57c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: /content/converted_labels/test_canonical.csv\n",
            "Total rows: 13019\n",
            "Unique unmapped tokens (sample up to 50): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_labels_for_training.py\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from src.utils import normalize_to_ipc   # use your utils if present\n",
        "import re\n",
        "\n",
        "IN_CSV = \"/content/converted_labels/dev_converted.csv\"   # your converted file (text, labels)\n",
        "OUT_CSV = \"/content/converted_labels/dev_canonical.csv\"\n",
        "\n",
        "df = pd.read_csv(IN_CSV, dtype=str).fillna('')\n",
        "# preserve raw\n",
        "df['labels_raw'] = df['labels'].astype(str)\n",
        "\n",
        "# helper to split semicolon separated raw labels into tokens\n",
        "def split_raw_labels(s):\n",
        "    if pd.isna(s) or str(s).strip()=='':\n",
        "        return []\n",
        "    toks = [t.strip() for t in re.split(r'[;,\\|]', str(s)) if t.strip()]\n",
        "    return toks\n",
        "\n",
        "# map tokens -> canonical using normalize_to_ipc\n",
        "def map_tokens_to_canonical(tokens):\n",
        "    mapped = []\n",
        "    unmapped = []\n",
        "    for t in tokens:\n",
        "        # first try if token already looks canonical (IPC_)\n",
        "        if isinstance(t, str) and t.upper().startswith('IPC_'):\n",
        "            mapped.append(t.upper())\n",
        "            continue\n",
        "        # use normalize_to_ipc which converts things like 'Section 120-B' or '120' -> 'IPC_120B' etc\n",
        "        cand = normalize_to_ipc(t)\n",
        "        if cand:\n",
        "            mapped.append(cand)\n",
        "        else:\n",
        "            unmapped.append(t)\n",
        "    # unique preserve order\n",
        "    seen = set(); out=[]\n",
        "    for m in mapped:\n",
        "        if m not in seen: out.append(m); seen.add(m)\n",
        "    return out, unmapped\n",
        "\n",
        "all_unmapped = []\n",
        "labels_canonical = []\n",
        "for _, row in df.iterrows():\n",
        "    toks = split_raw_labels(row['labels_raw'])\n",
        "    mapped, unmapped = map_tokens_to_canonical(toks)\n",
        "    labels_canonical.append(';'.join(mapped))\n",
        "    if unmapped:\n",
        "        all_unmapped.extend(unmapped)\n",
        "\n",
        "df['labels_canonical'] = labels_canonical\n",
        "\n",
        "# save\n",
        "Path(OUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(\"Wrote:\", OUT_CSV)\n",
        "print(\"Total rows:\", len(df))\n",
        "print(\"Unique unmapped tokens (sample up to 50):\", list(dict.fromkeys(all_unmapped))[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6P0KCAGB6FU",
        "outputId": "b9278421-b95b-4d3e-b231-206a660bd884"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: /content/converted_labels/dev_canonical.csv\n",
            "Total rows: 10181\n",
            "Unique unmapped tokens (sample up to 50): []\n"
          ]
        }
      ]
    }
  ]
}