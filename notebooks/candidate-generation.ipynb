{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown\n!gdown --folder \"https://drive.google.com/drive/folders/1pxqwBw2nSzZmPG4_B0huwW7nJJ-GlcNa?usp=sharing\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T04:33:55.586816Z","iopub.execute_input":"2025-12-04T04:33:55.587315Z","iopub.status.idle":"2025-12-04T04:34:43.800469Z","shell.execute_reply.started":"2025-12-04T04:33:55.587295Z","shell.execute_reply":"2025-12-04T04:34:43.799557Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.20.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.15.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.10.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\nRetrieving folder contents\nProcessing file 1dZ6mhxHvM_QvAdfSqWzb65U3zb2MY4uZ dev_canonical.csv\nProcessing file 1cwWovBZMYLHcwSr__2ocYx5VFpLQZ057 dev-00000-of-00001 (1).csv\nProcessing file 12HEI-zcyB2Yge1zkssjSxkRTzXUicPxi ipc_embeddings.npy\nProcessing file 1Cqned12VCLNTXPJPVGGJmdzaag9neEBc ipc_graph.pkl\nProcessing file 13m7_02-yGhs4SYEVCWydLrJ7NgYeFjdc ipc_sections.pkl\nProcessing file 1OEhEb10evvBqEmPbtXkZD_DbYlnj2O-f mlb_fixed.pkl\nProcessing file 1umpK4po91SlNXjsSBSUVAiSKC5Rtm10h Purushottam_Dashrath_Borate_vs_State_Of_Maharashtra_on_8_May_2015.PDF\nProcessing file 11FL5pyhEXLIweExurZTy_RAgC8KEdq97 statutes-00000-of-00001.csv\nProcessing file 1Mx58IzUN4AsLnFQ4T1yYpvNf7Ih3OFrf statutes.csv\nProcessing file 1u3I2b2voR5ZFAPmGWghjzf1cctXiBrQi test_canonical.csv\nProcessing file 1Qm8EJ_FVVSBoGnCdFg_7mfxY1ju8Nof5 test-00000-of-00001.csv\nProcessing file 1j7TA7Xt3GxgET9Hn4xN5sbDik9DADmsJ train_canonical_mlb_fixed.csv\nProcessing file 1hRxgbmJnrnEb-Hrai5uTMs64PQnrN75R train_canonical.csv\nProcessing file 1JRNGXElrBKDbIvoH1KsNmmEinMd3N1Qz train-00000-of-00001.csv\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1dZ6mhxHvM_QvAdfSqWzb65U3zb2MY4uZ\nTo: /kaggle/working/legal_dataset/dev_canonical.csv\n100%|███████████████████████████████████████| 78.5M/78.5M [00:00<00:00, 205MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1cwWovBZMYLHcwSr__2ocYx5VFpLQZ057\nTo: /kaggle/working/legal_dataset/dev-00000-of-00001 (1).csv\n100%|███████████████████████████████████████| 77.5M/77.5M [00:00<00:00, 122MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=12HEI-zcyB2Yge1zkssjSxkRTzXUicPxi\nTo: /kaggle/working/legal_dataset/ipc_embeddings.npy\n100%|███████████████████████████████████████| 1.40M/1.40M [00:00<00:00, 162MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Cqned12VCLNTXPJPVGGJmdzaag9neEBc\nTo: /kaggle/working/legal_dataset/ipc_graph.pkl\n100%|█████████████████████████████████████████| 529k/529k [00:00<00:00, 126MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=13m7_02-yGhs4SYEVCWydLrJ7NgYeFjdc\nTo: /kaggle/working/legal_dataset/ipc_sections.pkl\n100%|██████████████████████████████████████| 4.62k/4.62k [00:00<00:00, 21.9MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1OEhEb10evvBqEmPbtXkZD_DbYlnj2O-f\nTo: /kaggle/working/legal_dataset/mlb_fixed.pkl\n100%|██████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 11.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1umpK4po91SlNXjsSBSUVAiSKC5Rtm10h\nTo: /kaggle/working/legal_dataset/Purushottam_Dashrath_Borate_vs_State_Of_Maharashtra_on_8_May_2015.PDF\n100%|█████████████████████████████████████████| 282k/282k [00:00<00:00, 114MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=11FL5pyhEXLIweExurZTy_RAgC8KEdq97\nTo: /kaggle/working/legal_dataset/statutes-00000-of-00001.csv\n100%|██████████████████████████████████████| 47.4k/47.4k [00:00<00:00, 79.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Mx58IzUN4AsLnFQ4T1yYpvNf7Ih3OFrf\nTo: /kaggle/working/legal_dataset/statutes.csv\n100%|████████████████████████████████████████| 414k/414k [00:00<00:00, 83.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1u3I2b2voR5ZFAPmGWghjzf1cctXiBrQi\nTo: /kaggle/working/legal_dataset/test_canonical.csv\n100%|███████████████████████████████████████| 96.2M/96.2M [00:00<00:00, 102MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Qm8EJ_FVVSBoGnCdFg_7mfxY1ju8Nof5\nTo: /kaggle/working/legal_dataset/test-00000-of-00001.csv\n100%|███████████████████████████████████████| 94.9M/94.9M [00:00<00:00, 141MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1j7TA7Xt3GxgET9Hn4xN5sbDik9DADmsJ\nFrom (redirected): https://drive.google.com/uc?id=1j7TA7Xt3GxgET9Hn4xN5sbDik9DADmsJ&confirm=t&uuid=56c1c352-e954-428b-83ad-1d6c220705bc\nTo: /kaggle/working/legal_dataset/train_canonical_mlb_fixed.csv\n100%|█████████████████████████████████████████| 307M/307M [00:01<00:00, 198MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1hRxgbmJnrnEb-Hrai5uTMs64PQnrN75R\nFrom (redirected): https://drive.google.com/uc?id=1hRxgbmJnrnEb-Hrai5uTMs64PQnrN75R&confirm=t&uuid=b633669f-5e1a-47c6-b5a7-1e31a5baf259\nTo: /kaggle/working/legal_dataset/train_canonical.csv\n100%|████████████████████████████████████████| 292M/292M [00:03<00:00, 85.8MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1JRNGXElrBKDbIvoH1KsNmmEinMd3N1Qz\nFrom (redirected): https://drive.google.com/uc?id=1JRNGXElrBKDbIvoH1KsNmmEinMd3N1Qz&confirm=t&uuid=a7a85ce9-0313-4919-a895-1c80020589ae\nTo: /kaggle/working/legal_dataset/train-00000-of-00001.csv\n100%|█████████████████████████████████████████| 288M/288M [00:01<00:00, 154MB/s]\nDownload completed\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 0: (run only if you need to install/upgrade)\n# NOTE: keep versions generic to avoid dependency conflicts on Kaggle.\n# If transformers or torch are not present, uncomment and install. Usually Kaggle has them.\n!pip install -q transformers torch pandas tqdm scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T04:34:58.404003Z","iopub.execute_input":"2025-12-04T04:34:58.404585Z","iopub.status.idle":"2025-12-04T04:38:40.344552Z","shell.execute_reply.started":"2025-12-04T04:34:58.404546Z","shell.execute_reply":"2025-12-04T04:38:40.343594Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ---------- Cell 0 (run first) ----------\n# Monkeypatch to provide MessageFactory.GetPrototype for environments\n# where protobuf removed that symbol. This avoids the cryptic AttributeError.\n# Run this cell BEFORE importing transformers / sentence-transformers.\n\ntry:\n    from google.protobuf.message_factory import MessageFactory\n    # If GetPrototype is missing, create a shim that uses GetMessageClass (newer API)\n    if not hasattr(MessageFactory, \"GetPrototype\"):\n        # some protobuf versions provide GetMessageClass on the factory\n        if hasattr(MessageFactory, \"GetMessageClass\"):\n            def _get_prototype_shim(self, descriptor):\n                # GetMessageClass returns python class for descriptor\n                return self.GetMessageClass(descriptor)\n            MessageFactory.GetPrototype = _get_prototype_shim\n            print(\"Patched MessageFactory.GetPrototype -> GetMessageClass shim applied.\")\n        else:\n            # fallback shim using descriptor_pool (less likely needed)\n            from google.protobuf import descriptor_pool, message_factory, symbol_database\n            _pool = descriptor_pool.Default()\n            def _get_prototype_fallback(self, descriptor):\n                # create a new message class via symbol_database\n                db = symbol_database.Default()\n                try:\n                    # this may raise if not registered; attempt to register\n                    return db.GetPrototype(descriptor)\n                except Exception:\n                    # last resort: use message_factory to create a prototype\n                    return message_factory.MessageFactory().GetPrototype(descriptor)\n            MessageFactory.GetPrototype = _get_prototype_fallback\n            print(\"Patched MessageFactory.GetPrototype -> fallback shim applied.\")\n    else:\n        print(\"MessageFactory.GetPrototype already present; no patch needed.\")\nexcept Exception as e:\n    print(\"Could not apply protobuf MessageFactory patch (continuing). Exception:\", e)\n# ---------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:03:49.197035Z","iopub.execute_input":"2025-12-04T05:03:49.197780Z","iopub.status.idle":"2025-12-04T05:03:49.204093Z","shell.execute_reply.started":"2025-12-04T05:03:49.197755Z","shell.execute_reply":"2025-12-04T05:03:49.203496Z"}},"outputs":[{"name":"stdout","text":"Patched MessageFactory.GetPrototype -> fallback shim applied.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Cell 1 - imports & device\nimport os\nimport json\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional\nfrom tqdm.auto import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer\n\nprint(\"torch version:\", torch.__version__)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:04:03.884179Z","iopub.execute_input":"2025-12-04T05:04:03.884847Z","iopub.status.idle":"2025-12-04T05:04:03.889808Z","shell.execute_reply.started":"2025-12-04T05:04:03.884827Z","shell.execute_reply":"2025-12-04T05:04:03.889184Z"}},"outputs":[{"name":"stdout","text":"torch version: 2.6.0+cu124\nDevice: cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 2 - helpers: clean, embedder\nimport re\n\ndef clean_text(s: str) -> str:\n    if s is None:\n        return \"\"\n    s = str(s)\n    s = s.replace(\"\\u00A0\", \" \")\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\n# Embedding function using transformers + mean-pooling\nclass TransformerEmbedder:\n    def __init__(self, model_name: str = \"distilbert-base-uncased\", batch_size: int = 32):\n        self.model_name = model_name\n        self.batch_size = batch_size\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n        self.model.to(DEVICE)\n        self.model.eval()\n\n    def embed_texts(self, texts: List[str], show_progress: bool = True) -> np.ndarray:\n        \"\"\"\n        Return L2-normalized mean-pooled embeddings (numpy array shape (N, D))\n        \"\"\"\n        all_embs = []\n        bs = self.batch_size\n        texts = [clean_text(t) for t in texts]\n        for i in tqdm(range(0, len(texts), bs), disable=not show_progress, desc=\"Embedding\"):\n            batch = texts[i:i+bs]\n            enc = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n            input_ids = enc[\"input_ids\"].to(DEVICE)\n            attention_mask = enc[\"attention_mask\"].to(DEVICE)\n            with torch.no_grad():\n                out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n                # out.last_hidden_state: (B, L, D)\n                hidden = out.last_hidden_state\n                mask = attention_mask.unsqueeze(-1)\n                summed = (hidden * mask).sum(dim=1)    # (B, D)\n                lens = mask.sum(dim=1).clamp(min=1e-9) # (B,1)\n                mean_pooled = summed / lens\n                # convert to CPU numpy\n                emb = mean_pooled.cpu().numpy()\n                all_embs.append(emb)\n        all_embs = np.vstack(all_embs) if all_embs else np.zeros((0, self.model.config.hidden_size))\n        # L2-normalize\n        norms = np.linalg.norm(all_embs, axis=1, keepdims=True) + 1e-12\n        all_embs = all_embs / norms\n        return all_embs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:04:08.717082Z","iopub.execute_input":"2025-12-04T05:04:08.717880Z","iopub.status.idle":"2025-12-04T05:04:08.727100Z","shell.execute_reply.started":"2025-12-04T05:04:08.717855Z","shell.execute_reply":"2025-12-04T05:04:08.726422Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Cell 3 - load data and statutes\n# EDIT paths below to where your files live in Kaggle\nTRAIN_PATH = \"/kaggle/working/legal_dataset/train_canonical_mlb_fixed.csv\"   # <-- change\nDEV_PATH   = \"/kaggle/working/legal_dataset/dev_canonical.csv\"     # <-- change (optional)\nTEST_PATH  = \"/kaggle/working/legal_dataset/test_canonical.csv\"    # <-- change (optional)\nSTATUTES_PATH = \"/kaggle/working/legal_dataset/statutes.csv\"       # <-- change\n\ndef load_case_csv(path: str):\n    df = pd.read_csv(path, dtype=str).fillna('')\n    # Expected: columns: 'text' and 'labels' or 'labels_canonical'\n    if 'labels_canonical' in df.columns:\n        label_col = 'labels_canonical'\n    elif 'labels' in df.columns:\n        label_col = 'labels'\n    else:\n        # try other common names\n        label_col = [c for c in df.columns if 'label' in c.lower()]\n        label_col = label_col[0] if label_col else None\n    rows = []\n    for _, r in df.iterrows():\n        text = r.get('text') or r.get('case_text') or r.get('sentence') or ''\n        raw_labels = r.get(label_col, '') if label_col else ''\n        # normalize label string -> list of ids\n        labs = []\n        if isinstance(raw_labels, str) and raw_labels.strip():\n            s = raw_labels.strip()\n            # possible formats: \"IPC_302;IPC_120B\" or \"['IPC_302','IPC_120B']\" or \"IPC_302,IPC_120B\"\n            if s.startswith(\"[\") and s.endswith(\"]\"):\n                try:\n                    parsed = eval(s)  # safe-ish; relies on trusted dataset\n                    if isinstance(parsed, (list, tuple)):\n                        labs = [str(x).strip() for x in parsed if x]\n                except Exception:\n                    s2 = s.strip(\"[]\")\n                    labs = [x.strip().strip(\"'\\\"\") for x in re.split(r'[;,]', s2) if x.strip()]\n            else:\n                labs = [x.strip() for x in re.split(r'[;,]', s) if x.strip()]\n        rows.append({\"id\": r.get('id') or r.get('case_id') or None, \"text\": clean_text(text), \"labels\": labs})\n    return rows\n\nprint(\"Loading train/dev/test (if paths exist)...\")\ntrain = load_case_csv(TRAIN_PATH) if Path(TRAIN_PATH).exists() else []\ndev = load_case_csv(DEV_PATH) if Path(DEV_PATH).exists() else []\ntest = load_case_csv(TEST_PATH) if Path(TEST_PATH).exists() else []\n\nprint(\"Train examples:\", len(train), \"Dev:\", len(dev), \"Test:\", len(test))\n\n# Load statutes.csv and build id->text\nstat_df = pd.read_csv(STATUTES_PATH, dtype=str).fillna('')\n# Detect id/text columns heuristically\npossible_id_cols = [c for c in stat_df.columns if c.lower() in ('id','statute_id','code','section_id')]\npossible_text_cols = [c for c in stat_df.columns if c.lower() in ('description','def','offense','text','definition')]\nid_col = possible_id_cols[0] if possible_id_cols else stat_df.columns[0]\ntext_col = possible_text_cols[0] if possible_text_cols else stat_df.columns[1] if len(stat_df.columns)>1 else stat_df.columns[0]\n\nstatute_catalog = {}\nfor _, r in stat_df.iterrows():\n    sid = str(r[id_col]).strip()\n    stext = clean_text(str(r[text_col]))\n    if sid:\n        if not stext:\n            stext = sid\n        statute_catalog[sid] = stext\n\nprint(\"Loaded statute catalog size:\", len(statute_catalog))\n# If some statutes missing, we can fill them later with their ids as text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:04:18.656538Z","iopub.execute_input":"2025-12-04T05:04:18.657032Z","iopub.status.idle":"2025-12-04T05:04:50.581214Z","shell.execute_reply.started":"2025-12-04T05:04:18.657007Z","shell.execute_reply":"2025-12-04T05:04:50.580578Z"}},"outputs":[{"name":"stdout","text":"Loading train/dev/test (if paths exist)...\nTrain examples: 42750 Dev: 10181 Test: 13019\nLoaded statute catalog size: 454\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ------------------ Replacement Cell 4 (Transformer with TF-IDF fallback) ------------------\n# Purpose: Try to load a Hugging Face transformer embedder. If that fails (download/protobuf/401),\n# fall back to a TF-IDF embedder so the pipeline proceeds without HF downloads.\n\nfrom pathlib import Path\nimport traceback\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\nclass TfidfEmbedder:\n    \"\"\"\n    Fast fallback embedder using TF-IDF + SVD (optional).\n    Produces L2-normalized vectors compatible with subsequent pipeline.\n    \"\"\"\n    def __init__(self, texts: list[str], max_features: int = 20000, use_svd: bool = False, svd_dim: int = 256):\n        # texts: list of texts used to fit the TF-IDF (we'll fit using statute_texts + some queries if available)\n        self.vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=max_features, stop_words='english')\n        self.vectorizer.fit(texts)\n        self.use_svd = use_svd\n        if use_svd:\n            from sklearn.decomposition import TruncatedSVD\n            self.svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n            # we don't fit svd here; it will be fit on transform() call if needed\n\n    def embed_texts(self, texts: list[str], show_progress: bool = False) -> np.ndarray:\n        X = self.vectorizer.transform(texts).astype(np.float32)\n        if self.use_svd:\n            # fit-transform SVD on the fly (cheap for small data)\n            X = self.svd.fit_transform(X)\n        else:\n            X = X.toarray()\n        # L2 normalize\n        norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n        X = X / norms\n        return X\n\n# Try to initialize HF Transformer embedder; if anything fails, use TF-IDF fallback\nEMBED_MODEL = \"distilbert-base-uncased\"   # keep this default; change if you have local model path\nembedder = None\nuse_transformer = False\n\n# If you have a local folder containing model weights (uploaded to Kaggle dataset),\n# set LOCAL_MODEL_DIR = \"/kaggle/input/your-distilbert-folder\"\nLOCAL_MODEL_DIR = None  # set to path if you have a local HF model uploaded\n\ntry:\n    # attempt to import & create transformer embedder\n    if LOCAL_MODEL_DIR and Path(LOCAL_MODEL_DIR).exists():\n        # local model path provided\n        print(\"Trying to load transformer from local path:\", LOCAL_MODEL_DIR)\n        embedder = TransformerEmbedder(model_name=str(LOCAL_MODEL_DIR), batch_size=32)\n        use_transformer = True\n    else:\n        print(\"Trying to load transformer model from HuggingFace Hub:\", EMBED_MODEL)\n        embedder = TransformerEmbedder(model_name=EMBED_MODEL, batch_size=32)\n        use_transformer = True\n\n    # quick smoke test: embed a tiny dummy to make sure model fully loaded\n    _ = embedder.embed_texts([\"test embedding\"], show_progress=False)\n    print(\"Transformer embedder ready. Using transformer embeddings.\")\nexcept Exception as e:\n    print(\"Transformer embedder failed with exception (falling back to TF-IDF).\")\n    traceback.print_exc()\n    # Build TF-IDF fallback using statute_texts + all case texts available (so the TF-IDF vocabulary is useful)\n    all_texts_for_tfidf = []\n    # statute_texts variable must exist from earlier cell (Cell 3). If not, we attempt to build minimal list.\n    try:\n        # statute_texts should have been created in Cell 3\n        all_texts_for_tfidf.extend([t for t in statute_texts if t])\n    except Exception:\n        pass\n    # add some query texts from train/dev/test if present\n    try:\n        if len(train) > 0:\n            all_texts_for_tfidf.extend([ex['text'] for ex in train if ex.get('text')])\n        if len(dev) > 0:\n            all_texts_for_tfidf.extend([ex['text'] for ex in dev if ex.get('text')])\n        if len(test) > 0:\n            all_texts_for_tfidf.extend([ex['text'] for ex in test if ex.get('text')])\n    except Exception:\n        pass\n\n    # ensure we have something to fit\n    if not all_texts_for_tfidf:\n        all_texts_for_tfidf = (statute_texts if 'statute_texts' in globals() else [\" \"])\n    print(\"Fitting TF-IDF on\", len(all_texts_for_tfidf), \"documents\")\n    embedder = TfidfEmbedder(texts=all_texts_for_tfidf, max_features=20000, use_svd=False)\n    use_transformer = False\n    print(\"TF-IDF embedder ready (fallback).\")\n\n# Now compute statute embeddings using whichever embedder succeeded\nstatute_ids = list(statute_catalog.keys())\nstatute_texts = [statute_catalog[sid] for sid in statute_ids]\nprint(\"Number of statutes:\", len(statute_ids))\n\nprint(\"Computing statute embeddings...\")\nstatute_embs = embedder.embed_texts(statute_texts, show_progress=True)\nprint(\"Statute embeddings shape:\", statute_embs.shape)\n\n# Save artifacts\nout_dir = Path(\"reranker_artifacts\")\nout_dir.mkdir(exist_ok=True)\nnp.save(out_dir / \"statute_embs.npy\", statute_embs)\nwith open(out_dir / \"statute_ids.json\", \"w\") as f:\n    import json\n    json.dump(statute_ids, f)\nprint(\"Saved statute embeddings and ids to reranker_artifacts/\")\n\n# Helpful flag: whether we used transformer (True) or TF-IDF fallback (False)\nprint(\"Used transformer embeddings:\", use_transformer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T06:20:18.359502Z","iopub.execute_input":"2025-12-04T06:20:18.360136Z","iopub.status.idle":"2025-12-04T06:20:33.005637Z","shell.execute_reply.started":"2025-12-04T06:20:18.360092Z","shell.execute_reply":"2025-12-04T06:20:33.005035Z"}},"outputs":[{"name":"stdout","text":"Trying to load transformer model from HuggingFace Hub: distilbert-base-uncased\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dabae6251b67436eb15055a9d218b4c5"}},"metadata":{}},{"name":"stdout","text":"Transformer embedder ready. Using transformer embeddings.\nNumber of statutes: 454\nComputing statute embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24543aa31a2a4dc99e4f7f9a8ca62c04"}},"metadata":{}},{"name":"stdout","text":"Statute embeddings shape: (454, 768)\nSaved statute embeddings and ids to reranker_artifacts/\nUsed transformer embeddings: True\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 5 - dense retrieval candidate generation\ndef retrieve_topk_dense(query: str, query_emb: np.ndarray, statute_embs: np.ndarray, topk: int = 50):\n    \"\"\"\n    query_emb: normalized vector shape (D,) or (1,D)\n    statute_embs: normalized (N, D)\n    returns list of (idx, score) sorted desc\n    \"\"\"\n    # dot-product since embeddings are normalized => cosine\n    scores = np.dot(statute_embs, query_emb.reshape(-1))\n    topk_idx = np.argsort(scores)[::-1][:topk]\n    return [(int(i), float(scores[i])) for i in topk_idx]\n\n# convenience: embed a single query\ndef embed_single_query(text: str):\n    embs = embedder.embed_texts([text], show_progress=False)\n    return embs[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T06:20:48.952618Z","iopub.execute_input":"2025-12-04T06:20:48.953252Z","iopub.status.idle":"2025-12-04T06:20:48.958272Z","shell.execute_reply.started":"2025-12-04T06:20:48.953226Z","shell.execute_reply":"2025-12-04T06:20:48.957466Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Cell 6 - create pairwise training dataset\ndef create_pairwise_examples(cases: List[dict], statute_ids: List[str], statute_embs: np.ndarray, topk=50, neg_per_pos=3):\n    \"\"\"\n    Return list of dicts: {'query_text':..., 'q_emb':np.array, 'cand_idx':int, 'cand_emb':np.array, 'label':0/1}\n    \"\"\"\n    examples = []\n    for ex in tqdm(cases, desc=\"build_pairs\"):\n        qtext = ex['text']\n        gold = set([g for g in ex['labels'] if g in statute_catalog])\n        if not qtext:\n            continue\n        q_emb = embed_single_query(qtext)\n        # retrieve topk\n        cand_idx_scores = retrieve_topk_dense(qtext, q_emb, statute_embs, topk=topk)\n        cand_indices = [i for i,_ in cand_idx_scores]\n        # positives among retrieved\n        pos_indices = [i for i in cand_indices if statute_ids[i] in gold]\n        # if no positives among topk, still add one positive if some gold in catalog (helps training)\n        if not pos_indices and gold:\n            for g in gold:\n                if g in statute_ids:\n                    pos_indices.append(statute_ids.index(g))\n        # negatives pool\n        neg_pool = [i for i in cand_indices if i not in pos_indices]\n        # sample negatives\n        for p in pos_indices:\n            examples.append({\"q_text\": qtext, \"q_emb\": q_emb, \"cand_idx\": p, \"cand_emb\": statute_embs[p], \"label\": 1})\n            # sample negatives for this positive\n            n_take = min(len(neg_pool), neg_per_pos)\n            if n_take > 0:\n                sampled = random.sample(neg_pool, n_take)\n                for ni in sampled:\n                    examples.append({\"q_text\": qtext, \"q_emb\": q_emb, \"cand_idx\": ni, \"cand_emb\": statute_embs[ni], \"label\": 0})\n    random.shuffle(examples)\n    return examples\n\nprint(\"Creating training pairs from train split...\")\ntrain_pairs = create_pairwise_examples(train, statute_ids, statute_embs, topk=50, neg_per_pos=3)\nprint(\"Train pairs:\", len(train_pairs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T06:21:00.198135Z","iopub.execute_input":"2025-12-04T06:21:00.198858Z","iopub.status.idle":"2025-12-04T06:30:02.677511Z","shell.execute_reply.started":"2025-12-04T06:21:00.198832Z","shell.execute_reply":"2025-12-04T06:30:02.676831Z"}},"outputs":[{"name":"stdout","text":"Creating training pairs from train split...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"build_pairs:   0%|          | 0/42750 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ce0624a1dd4448a70e8af3ed558c03"}},"metadata":{}},{"name":"stdout","text":"Train pairs: 439004\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 7 - dataset and dataloader\nclass PairwiseDataset(Dataset):\n    def __init__(self, pairs):\n        self.pairs = pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        it = self.pairs[idx]\n        # return numpy arrays; convert to float32\n        return {\n            \"q_emb\": it[\"q_emb\"].astype(np.float32),\n            \"cand_emb\": it[\"cand_emb\"].astype(np.float32),\n            \"label\": float(it[\"label\"])\n        }\n\ndef collate_pairs(batch):\n    q_emb = torch.tensor([b[\"q_emb\"] for b in batch], dtype=torch.float32)\n    cand_emb = torch.tensor([b[\"cand_emb\"] for b in batch], dtype=torch.float32)\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.float32).unsqueeze(1)\n    return q_emb.to(DEVICE), cand_emb.to(DEVICE), labels.to(DEVICE)\n\nbatch_size = 64\ntrain_dataset = PairwiseDataset(train_pairs)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pairs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T06:35:07.015902Z","iopub.execute_input":"2025-12-04T06:35:07.016616Z","iopub.status.idle":"2025-12-04T06:35:07.022942Z","shell.execute_reply.started":"2025-12-04T06:35:07.016591Z","shell.execute_reply":"2025-12-04T06:35:07.022178Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Cell 8 - MLP reranker model\nclass MLP_Reranker(nn.Module):\n    def __init__(self, emb_dim: int, hidden_dim: int = 512, dropout: float = 0.2):\n        super().__init__()\n        # Input is [q_emb, cand_emb, abs_diff, elementwise_prod] concatenated\n        in_dim = emb_dim * 4\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim//2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim//2, 1)  # output score (logit)\n        )\n\n    def forward(self, q_emb, cand_emb):\n        # q_emb, cand_emb: (B, D)\n        diff = torch.abs(q_emb - cand_emb)\n        prod = q_emb * cand_emb\n        x = torch.cat([q_emb, cand_emb, diff, prod], dim=1)\n        out = self.net(x)\n        return out\n\nemb_dim = statute_embs.shape[1]\nmodel = MLP_Reranker(emb_dim, hidden_dim=512, dropout=0.2).to(DEVICE)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T06:35:16.408790Z","iopub.execute_input":"2025-12-04T06:35:16.409369Z","iopub.status.idle":"2025-12-04T06:35:16.434982Z","shell.execute_reply.started":"2025-12-04T06:35:16.409344Z","shell.execute_reply":"2025-12-04T06:35:16.434382Z"}},"outputs":[{"name":"stdout","text":"MLP_Reranker(\n  (net): Sequential(\n    (0): Linear(in_features=3072, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=512, out_features=256, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Cell 9 - training loop\nimport torch.optim as optim\nfrom sklearn.metrics import roc_auc_score\n\ndef train_reranker(model, train_loader, epochs=6, lr=2e-4, val_loader=None):\n    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n    criterion = nn.BCEWithLogitsLoss()\n    best_loss = 1e9\n    for ep in range(1, epochs+1):\n        model.train()\n        total_loss = 0.0\n        for q_emb, cand_emb, labels in tqdm(train_loader, desc=f\"Train ep{ep}\"):\n            opt.zero_grad()\n            logits = model(q_emb, cand_emb)\n            loss = criterion(logits, labels)\n            loss.backward()\n            opt.step()\n            total_loss += loss.item() * q_emb.size(0)\n        avg_loss = total_loss / len(train_loader.dataset)\n        print(f\"Epoch {ep} train_loss: {avg_loss:.6f}\")\n        # optional: evaluate on small validation set for monitoring\n    return model\n\n# Train (adjust epochs based on resources)\nmodel = train_reranker(model, train_loader, epochs=6, lr=2e-4)\n# Save model\ntorch.save(model.state_dict(), out_dir / \"mlp_reranker_state.pth\")\nprint(\"Saved reranker state.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T06:35:26.823588Z","iopub.execute_input":"2025-12-04T06:35:26.823927Z","iopub.status.idle":"2025-12-04T06:46:59.573936Z","shell.execute_reply.started":"2025-12-04T06:35:26.823906Z","shell.execute_reply":"2025-12-04T06:46:59.573149Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Train ep1:   0%|          | 0/6860 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f906e88f120417184dd4498070e82ff"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_47/3348135988.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  q_emb = torch.tensor([b[\"q_emb\"] for b in batch], dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 train_loss: 0.124271\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train ep2:   0%|          | 0/6860 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfbbc066e91743dc93df370dfd1af535"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 train_loss: 0.077559\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train ep3:   0%|          | 0/6860 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e7eebef2fa041f8927a366b68d11538"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 train_loss: 0.070487\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train ep4:   0%|          | 0/6860 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d366dbaead194476b86a946639ab051e"}},"metadata":{}},{"name":"stdout","text":"Epoch 4 train_loss: 0.066755\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train ep5:   0%|          | 0/6860 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20fdc4130dfd411ba80fe9fad6bc5dad"}},"metadata":{}},{"name":"stdout","text":"Epoch 5 train_loss: 0.063722\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train ep6:   0%|          | 0/6860 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e41514c7271445958a369af336d0f37f"}},"metadata":{}},{"name":"stdout","text":"Epoch 6 train_loss: 0.062060\nSaved reranker state.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Cell 10 - evaluation utilities\ndef reranker_score_for_query(model, q_emb: np.ndarray, candidate_indices: List[int]):\n    \"\"\"\n    Given a single query embedding and list of candidate indices, compute reranker scores.\n    Returns list of (idx, score)\n    \"\"\"\n    model.eval()\n    qs = np.repeat(q_emb.reshape(1,-1), len(candidate_indices), axis=0).astype(np.float32)\n    cs = statute_embs[candidate_indices].astype(np.float32)\n    with torch.no_grad():\n        q_t = torch.tensor(qs, dtype=torch.float32).to(DEVICE)\n        c_t = torch.tensor(cs, dtype=torch.float32).to(DEVICE)\n        logits = model(q_t, c_t).cpu().numpy().reshape(-1)\n    return list(zip(candidate_indices, logits.tolist()))\n\ndef evaluate_split(split_cases: List[dict], statute_ids: List[str], statute_embs: np.ndarray,\n                   model, topk=100, rerank_k=10):\n    recall_hits = 0\n    rr_total = 0.0\n    count_q = 0\n    for ex in tqdm(split_cases, desc=\"Eval\"):\n        qtext = ex['text']\n        gold = set([g for g in ex['labels'] if g in statute_catalog])\n        if not qtext or len(gold)==0:\n            continue\n        count_q += 1\n        q_emb = embed_single_query(qtext)\n        cand_idx_scores = retrieve_topk_dense(qtext, q_emb, statute_embs, topk=topk)\n        cand_indices = [i for i,_ in cand_idx_scores]\n        # rerank using model\n        reranked = reranker_score_for_query(model, q_emb, cand_indices)\n        # sort by score desc\n        ranked = sorted(reranked, key=lambda x: x[1], reverse=True)\n        topk_ranked = [statute_ids[idx] for idx,_ in ranked[:rerank_k]]\n        if any(sid in gold for sid in topk_ranked):\n            recall_hits += 1\n        # compute reciprocal rank\n        rr = 0.0\n        for rank_pos, (idx, _) in enumerate(ranked, start=1):\n            sid = statute_ids[idx]\n            if sid in gold:\n                rr = 1.0 / rank_pos\n                break\n        rr_total += rr\n    recall_at_k = recall_hits / count_q if count_q>0 else 0.0\n    mrr = rr_total / count_q if count_q>0 else 0.0\n    return {\"recall@{}\".format(rerank_k): recall_at_k, \"mrr\": mrr, \"n_queries\": count_q}\n\n# Evaluate on dev and test (if present)\nif len(dev)>0:\n    dev_metrics = evaluate_split(dev, statute_ids, statute_embs, model, topk=100, rerank_k=10)\n    print(\"Dev metrics:\", dev_metrics)\nif len(test)>0:\n    test_metrics = evaluate_split(test, statute_ids, statute_embs, model, topk=100, rerank_k=10)\n    print(\"Test metrics:\", test_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T06:47:21.875373Z","iopub.execute_input":"2025-12-04T06:47:21.875968Z","iopub.status.idle":"2025-12-04T06:52:38.757445Z","shell.execute_reply.started":"2025-12-04T06:47:21.875945Z","shell.execute_reply":"2025-12-04T06:52:38.756640Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/10181 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"374c7eefdbae443daea7a1fde18a1ab0"}},"metadata":{}},{"name":"stdout","text":"Dev metrics: {'recall@10': 0.390479000688502, 'mrr': 0.21090147599864018, 'n_queries': 10167}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/13019 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84bfc35b25cc4742b8bd40f828c43379"}},"metadata":{}},{"name":"stdout","text":"Test metrics: {'recall@10': 0.4097831102907245, 'mrr': 0.21605110016625156, 'n_queries': 13002}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Cell 11 - save artifacts and demo\n# Save tokenizer/model metadata (we reuse embedding model; saving reranker separately)\ntorch.save({\n    \"statute_ids\": statute_ids,\n    \"statute_texts\": statute_texts\n}, out_dir / \"statute_catalog.pt\")\n\n# Save embedder model name\nwith open(out_dir / \"embedder_info.json\", \"w\") as f:\n    json.dump({\"embed_model\": EMBED_MODEL, \"emb_dim\": int(statute_embs.shape[1])}, f)\n\nprint(\"Artifacts saved in\", out_dir)\n\n# Demo inference function\ndef predict_for_text(query: str, topk=20, rerank_k=10):\n    q_emb = embed_single_query(query)\n    dense_candidates = retrieve_topk_dense(query, q_emb, statute_embs, topk=topk)\n    cand_indices = [i for i,_ in dense_candidates]\n    reranked = reranker_score_for_query(model, q_emb, cand_indices)\n    ranked = sorted(reranked, key=lambda x: x[1], reverse=True)\n    out = []\n    for idx, score in ranked[:rerank_k]:\n        out.append({\"statute_id\": statute_ids[idx], \"statute_text\": statute_catalog[statute_ids[idx]], \"score\": float(score)})\n    return out\n\n# Example\nif len(train)>0:\n    sample_q = train[0]['text']\n    print(\"Sample query (truncated):\", sample_q[:400])\n    preds = predict_for_text(sample_q, topk=50, rerank_k=10)\n    for p in preds:\n        print(p['statute_id'], \" score:\", p['score'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T06:53:41.413525Z","iopub.execute_input":"2025-12-04T06:53:41.413816Z","iopub.status.idle":"2025-12-04T06:53:41.446875Z","shell.execute_reply.started":"2025-12-04T06:53:41.413788Z","shell.execute_reply":"2025-12-04T06:53:41.446151Z"}},"outputs":[{"name":"stdout","text":"Artifacts saved in reranker_artifacts\nSample query (truncated): ['(a), <SECTION> of the <ACT>. The gist of theprosecution case relevant <ENTITY> the purpose of this proceedingmay be stated thus: With the growth of industry, commerceand trade in and around the city of Mumbai which generatessubstantial quantity of wealth, there has been increase oforganised activities by gangs of anti-socials to extractmoney from affluent sections of society like developers,hote\nIPC_376  score: -1.1044032573699951\nIPC_304B  score: -1.5030287504196167\nIPC_195  score: -4.679609298706055\nIPC_123  score: -4.722894668579102\nIPC_376A  score: -5.146350860595703\nIPC_170  score: -5.626493453979492\nIPC_127  score: -5.887179374694824\nIPC_418  score: -6.628131866455078\nIPC_194  score: -7.077974796295166\nIPC_205  score: -7.2961835861206055\n","output_type":"stream"}],"execution_count":24}]}